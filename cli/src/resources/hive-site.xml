<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- hive元数据的存储位置 -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://192.168.1.9:3306/hivemetadata?createDatabaseIfNotExist=true&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>
    <!-- 指定驱动程序 -->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>
    <!-- 连接数据库的用户名 -->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
        <description>username to use against metastore database</description>
    </property>
    <!-- 连接数据库的口令 -->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>12345678</value>
        <description>password to use against metastore database</description>
    </property>
    <property>
        <!-- 数据默认的存储位置(HDFS) -->
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
        <description>location of default database for the
            warehouse</description>
    </property>
    <property>
        <!-- 在命令行中，显示当前操作的数据库 -->
        <name>hive.cli.print.current.db</name>
        <value>true</value>
        <description>Whether to include the current database in
            the Hive prompt.</description>
    </property>
    <property>
        <!-- 在命令行中，显示数据的表头 -->
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>
    <property>
        <!-- 操作小规模数据时，使用本地模式，提高效率 -->
        <name>hive.exec.mode.local.auto</name>
        <value>true</value>
        <description>Let Hive determine whether to run in local mode automatically</description>
    </property>
    <property>
        <name>spark.home</name>
        <value>/root/hadoop/spark-2.3.0-bin-hadoop2.7</value>
    </property>
    <property>
        <name>spark.yarn.jars</name>
        <value>hdfs://192.168.1.9:8020/spark-yarn/jars/*</value>
    </property>
<!--    <property>-->
<!--        <name>hive.metastore.uris</name>-->
<!--        <value>thrift://hadoop001:9083</value>-->
<!--    </property>-->
    <property>
        <name>datanucleus.schema.autocreateall</name>
        <value>false</value>
    </property>
<!--    <property>-->
<!--        <name>hive.metastore.local</name>-->
<!--        <value>true</value>-->
<!--    </property>-->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://192.168.1.9:9083</value>   <!-- 配置hive 中metastore启动ip和端口-->
    </property>
    <property>
        <name>hive.jar.path</name>
        <value>D:\apache-hive-2.3.6-src\packaging\target\apache-hive-2.3.6-bin\apache-hive-2.3.6-bin\lib\hive-cli-2.3.6.jar</value>
        <!-- 配置hive 中metastore启动ip和端口-->
    </property>
</configuration>
